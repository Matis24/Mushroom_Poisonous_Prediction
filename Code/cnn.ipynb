{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the dataset of 15 family of mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/mbrei/OneDrive/Bureau/ChampIA/Data/data_image\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class to get the path of each image according to family and data type to transform it in tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MushroomDataset(Dataset):\n",
    "        def __init__(self, root_dir, data_type, transform=None):\n",
    "                \"\"\"\n",
    "                Choisir un répertoire racine et une transformation à appliquer aux images\n",
    "                \"\"\"\n",
    "                self.root_dir = root_dir\n",
    "                self.data_type = data_type \n",
    "                self.transform = transform\n",
    "                self.data = []\n",
    "                self.classes = {}\n",
    "                self._load_images()\n",
    "                \n",
    "        def _load_images(self):\n",
    "                \"\"\"\n",
    "                Récupérer les chemins de chaque image via différents dossiers avec l'étiquette de la famille de champignons\n",
    "                \"\"\"\n",
    "                class_idx = 0\n",
    "                for family_name in os.listdir(self.root_dir):\n",
    "                        family_dir = os.path.join(self.root_dir, family_name)\n",
    "                        if os.path.isdir(family_dir):\n",
    "                                data_type_dir = os.path.join(family_dir, 'data', self.data_type, family_name).replace('\\\\', '/')\n",
    "                                data_type_dir_champ = os.path.join(family_dir, 'data', self.data_type, f\"mushrooms {family_name}\").replace('\\\\', '/')\n",
    "                                if os.path.exists(data_type_dir):\n",
    "                                        target_dir = data_type_dir\n",
    "                                elif os.path.exists(data_type_dir_champ):\n",
    "                                        target_dir = data_type_dir_champ\n",
    "                                else:\n",
    "                                        continue\n",
    "                                if family_name not in self.classes:\n",
    "                                        self.classes[family_name] = class_idx\n",
    "                                        class_idx += 1\n",
    "                                for img_file in os.listdir(target_dir):\n",
    "                                        img_path = os.path.join(target_dir, img_file).replace('\\\\', '/')\n",
    "                                        if img_file.lower().endswith(('.jpg')):\n",
    "                                                self.data.append((img_path, family_name))\n",
    "        \n",
    "        def __len__(self):\n",
    "                return len(self.data)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "                img_path, family_name = self.data[idx]\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                \n",
    "                if self.transform:\n",
    "                        image = self.transform(image)\n",
    "                \n",
    "                # Convertir le nom de famille en index numérique\n",
    "                label = self.classes[family_name]\n",
    "                \n",
    "                return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with only train\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_data = MushroomDataset(root_dir=path, data_type='train', transform=transform)\n",
    "test_data = MushroomDataset(root_dir=path, data_type='test', transform=transform)\n",
    "val_data = MushroomDataset(root_dir=path, data_type='val', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8471, 0.7647, 0.7608,  ..., 0.4745, 0.3922, 0.3176],\n",
       "          [0.7843, 0.7098, 0.7765,  ..., 0.3529, 0.3333, 0.2353],\n",
       "          [0.7765, 0.7294, 0.6471,  ..., 0.4824, 0.4275, 0.3765],\n",
       "          ...,\n",
       "          [0.5294, 0.5647, 0.3843,  ..., 0.5294, 0.3686, 0.2275],\n",
       "          [0.3373, 0.3216, 0.3804,  ..., 0.1725, 0.1451, 0.4275],\n",
       "          [0.3020, 0.2157, 0.1216,  ..., 0.3294, 0.4706, 0.5059]],\n",
       " \n",
       "         [[0.8353, 0.7176, 0.6706,  ..., 0.4078, 0.3686, 0.3216],\n",
       "          [0.7569, 0.6549, 0.6784,  ..., 0.2588, 0.2667, 0.1843],\n",
       "          [0.7294, 0.6549, 0.5451,  ..., 0.3569, 0.3059, 0.2510],\n",
       "          ...,\n",
       "          [0.3373, 0.3765, 0.1882,  ..., 0.4784, 0.2667, 0.0706],\n",
       "          [0.1725, 0.1569, 0.2196,  ..., 0.1216, 0.0667, 0.3216],\n",
       "          [0.1569, 0.0784, 0.0078,  ..., 0.2549, 0.4196, 0.4824]],\n",
       " \n",
       "         [[0.6745, 0.6314, 0.6392,  ..., 0.3294, 0.3137, 0.2902],\n",
       "          [0.6471, 0.6039, 0.6627,  ..., 0.2510, 0.2902, 0.2235],\n",
       "          [0.6824, 0.6392, 0.5490,  ..., 0.4000, 0.3765, 0.3412],\n",
       "          ...,\n",
       "          [0.2078, 0.2353, 0.0431,  ..., 0.4549, 0.2706, 0.1020],\n",
       "          [0.1255, 0.0941, 0.1412,  ..., 0.0980, 0.0392, 0.2863],\n",
       "          [0.1882, 0.0941, 0.0000,  ..., 0.2314, 0.3529, 0.3804]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 128, 128]), 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_data.__getitem__(1200)\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation des images pour les amener à la bonne forme et les normaliser\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize((128, 128)),  # Assurer que l'image est bien de taille 128x128\\n\",\n",
    "  transforms.ToTensor(),  # Convertir l'image en un tenseur\\n\",\n",
    "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalisation\\n\",\n",
    "  ]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# Supposons que train_data est un objet Dataset personnalisé, donc on l'enveloppe dans un DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "\n",
    "        def __init__(self, num_classes):\n",
    "            super(SimpleCNN, self).__init__()\n",
    "            \n",
    "            # Définir les couches du CNN\n",
    "            self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "            \n",
    "            # MaxPooling pour réduire la taille de l'image\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            \n",
    "            # Couches Fully Connected\n",
    "            self.fc1 = nn.Linear(64 * 16 * 16, 512)  # Assurez-vous que les dimensions correspondent\n",
    "            self.fc2 = nn.Linear(512, num_classes)  # Nombre de classes\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Appliquer les convolutions et les max pooling\n",
    "            x = self.pool(torch.relu(self.conv1(x)))\n",
    "            x = self.pool(torch.relu(self.conv2(x)))\n",
    "            x = self.pool(torch.relu(self.conv3(x)))\n",
    "            # Applatir l'image pour la passer dans les couches fully connected\n",
    "            x = x.view(-1, 64 * 16 * 16)\n",
    "            \n",
    "            # Appliquer les couches fully connected\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"runs/experiment3\")\n",
    "# writer.add_text(\"Learning rate de 0\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.005\n",
    "EPOCH = 15\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_data.classes)\n",
    "model = SimpleCNN(num_classes=num_classes)\n",
    "hparams = {'lr': LR, 'batch_size': BATCH_SIZE, 'optimizer': 'Adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleCNN                                [16, 13]                  --\n",
       "├─Conv2d: 1-1                            [16, 16, 128, 128]        448\n",
       "├─MaxPool2d: 1-2                         [16, 16, 64, 64]          --\n",
       "├─Conv2d: 1-3                            [16, 32, 64, 64]          4,640\n",
       "├─MaxPool2d: 1-4                         [16, 32, 32, 32]          --\n",
       "├─Conv2d: 1-5                            [16, 64, 32, 32]          18,496\n",
       "├─MaxPool2d: 1-6                         [16, 64, 16, 16]          --\n",
       "├─Linear: 1-7                            [16, 512]                 8,389,120\n",
       "├─Linear: 1-8                            [16, 13]                  6,669\n",
       "==========================================================================================\n",
       "Total params: 8,419,373\n",
       "Trainable params: 8,419,373\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 858.90\n",
       "==========================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 58.79\n",
       "Params size (MB): 33.68\n",
       "Estimated Total Size (MB): 95.61\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!python -m tensorboard.main --logdir=Code/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Loss: 2.4628 - Acc: 11.7197 | Val Loss: 2.4536 - Val Acc: 12.8306 | \n",
      "Epoch 2/15 - Loss: 2.4436 - Acc: 11.7056 | Val Loss: 2.4530 - Val Acc: 12.8306 | \n",
      "Epoch 3/15 - Loss: 2.4441 - Acc: 12.1302 | Val Loss: 2.4511 - Val Acc: 12.8306 | \n",
      "Epoch 4/15 - Loss: 2.4415 - Acc: 11.9887 | Val Loss: 2.4546 - Val Acc: 12.8306 | \n",
      "Epoch 5/15 - Loss: 2.4407 - Acc: 12.4841 | Val Loss: 2.4573 - Val Acc: 11.1986 | \n",
      "Epoch 6/15 - Loss: 2.4405 - Acc: 12.1019 | Val Loss: 2.4520 - Val Acc: 12.8306 | \n",
      "Epoch 7/15 - Loss: 2.4416 - Acc: 12.0736 | Val Loss: 2.4540 - Val Acc: 12.8306 | \n",
      "Epoch 8/15 - Loss: 2.4411 - Acc: 12.5548 | Val Loss: 2.4515 - Val Acc: 12.8306 | \n",
      "Epoch 9/15 - Loss: 2.4395 - Acc: 12.2435 | Val Loss: 2.4538 - Val Acc: 12.8306 | \n",
      "Epoch 10/15 - Loss: 2.4397 - Acc: 12.3992 | Val Loss: 2.4499 - Val Acc: 12.8306 | \n",
      "Epoch 11/15 - Loss: 2.4402 - Acc: 12.5548 | Val Loss: 2.4528 - Val Acc: 12.8306 | \n",
      "Epoch 12/15 - Loss: 2.4394 - Acc: 12.5548 | Val Loss: 2.4536 - Val Acc: 12.8306 | \n",
      "Epoch 13/15 - Loss: 2.4391 - Acc: 12.3142 | Val Loss: 2.4506 - Val Acc: 12.8306 | \n",
      "Epoch 14/15 - Loss: 2.4394 - Acc: 12.5548 | Val Loss: 2.4534 - Val Acc: 12.8306 | \n",
      "Epoch 15/15 - Loss: 2.4392 - Acc: 12.4558 | Val Loss: 2.4517 - Val Acc: 12.8306 | \n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Définir l'appareil (GPU ou CPU)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Initialisation des meilleures accuracies\n",
    "best_train_acc = 0.0\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Entraîner le modèle avec validation\n",
    "for epoch in range(EPOCH):\n",
    "    # Phase d'entraînement\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    # Phase de validation\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_epoch_loss = val_loss / len(val_loader)\n",
    "    val_epoch_acc = 100 * val_correct / val_total\n",
    "\n",
    "    # Mettre à jour les meilleures accuracies\n",
    "    if epoch_acc > best_train_acc:\n",
    "        best_train_acc = epoch_acc\n",
    "\n",
    "    if val_epoch_acc > best_val_acc:\n",
    "        best_val_acc = val_epoch_acc\n",
    "\n",
    "    # Affichage des métriques\n",
    "    print(f\"Epoch {epoch+1}/{EPOCH} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f} - Val Acc: {val_epoch_acc:.4f} | \")\n",
    "    \n",
    "    # Enregistrement dans TensorBoard\n",
    "    writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", epoch_acc, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", val_epoch_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", val_epoch_acc, epoch)\n",
    "\n",
    "\n",
    "description = f\"\"\"\n",
    "# Expérience d'entraînement\n",
    "\n",
    "## Paramètres :\n",
    "- Learning Rate : {LR}\n",
    "- Batch Size : {BATCH_SIZE}\n",
    "- Optimiseur : {optimizer}\n",
    "\n",
    "## Meilleures Performances :\n",
    "- Best Train Accuracy : {best_train_acc:.4f}%\n",
    "- Best Validation Accuracy : {best_val_acc:.4f}%\n",
    "\n",
    "## Architecture du modèle :\n",
    "{summary(model, input_size=(BATCH_SIZE, 3, 128, 128))}\n",
    "\"\"\"\n",
    "\n",
    "writer.add_text(\"Documentation du modèle :\", description)\n",
    "    \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./Model/mushroom_cnn_001.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test data: 42.86%\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(model, test_loader, device):\n",
    "    model.eval()  # Mettre le modèle en mode évaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Désactiver la gradation\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test data: {accuracy:.2f}%')\n",
    "\n",
    "test_accuracy(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
